---
title: "Bayesian-XGBoost"
layout: post
excerpt: ""
categories:
  - R Tutorial
  - Reproducible Example
tags:
  - XGBoost
  - Gradient Boosting
  - Supervised Learning
  - GBM
last_modified_at: 2019-08-02T12:43:31-05:00
---

Machine learning models learn by looking at examples. In supervised learning, there is a known outcome, or label, and by looking at examples of data across many, many examples, a relationship established between the data and the label. More examples makes it easier for the model to find patterns; fewer examples makes it more difficult to learn, and hence, leads to worse performance.

What can be done when there are few examples to learn from?  This tutorial demonstrates how to use prior information to fill in these gaps in the data to improve model performance.

# [See The Tutorial](https://nbviewer.jupyter.org/github/sdcastillo/Prior-Weights-with-XGboost/blob/master/xgb_with_prior.html)
